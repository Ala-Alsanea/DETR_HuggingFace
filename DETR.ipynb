{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d2864a2",
      "metadata": {
        "id": "5d2864a2"
      },
      "outputs": [],
      "source": [
        "!pip3 install transformers timm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32792096",
      "metadata": {
        "id": "32792096",
        "outputId": "259a4ec8-c18e-48ad-8d9a-28af3a62aa5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/uwu/Desktop/object_detection/preview/DETR_HuggingFace\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "HOME = os.getcwd()\n",
        "print(HOME)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "063c1463",
      "metadata": {
        "id": "063c1463"
      },
      "source": [
        "### Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7582dda3",
      "metadata": {
        "id": "7582dda3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import DetrForObjectDetection, DetrImageProcessor\n",
        "\n",
        "\n",
        "# settings\n",
        "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "CHECKPOINT = 'facebook/detr-resnet-50'\n",
        "CONFIDENCE_TRESHOLD = 0.5\n",
        "IOU_TRESHOLD = 0.8\n",
        "\n",
        "image_processor = DetrImageProcessor.from_pretrained(CHECKPOINT)\n",
        "model = DetrForObjectDetection.from_pretrained(CHECKPOINT)\n",
        "model.to(DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e9b7c00",
      "metadata": {
        "id": "5e9b7c00"
      },
      "source": [
        "# load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "818599d0",
      "metadata": {
        "id": "818599d0"
      },
      "outputs": [],
      "source": [
        " !git clone https://github.com/Ala-Alsanea/sketch_web_ui_dataset.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd sketch_web_ui_dataset &&  python3 setup.py"
      ],
      "metadata": {
        "id": "9dCw2ah76fv8"
      },
      "id": "9dCw2ah76fv8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 sketch_web_ui_dataset/create_new_dataset_and_preprocess.py -p sketch_web_ui_dataset"
      ],
      "metadata": {
        "id": "zEd3jYgF6fso"
      },
      "id": "zEd3jYgF6fso",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72406ed7",
      "metadata": {
        "id": "72406ed7",
        "outputId": "20e7f0b6-d54b-435d-96c9-737ff14abdaa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sketch_web_ui_dataset/train/images/annotations.coco.json\n",
            "loading annotations into memory...\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'sketch_web_ui_dataset/train/images/annotations.coco.json'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[3], line 35\u001b[0m\n\u001b[1;32m     30\u001b[0m         target \u001b[38;5;241m=\u001b[39m encoding[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m pixel_values, target\n\u001b[0;32m---> 35\u001b[0m TRAIN_DATASET \u001b[38;5;241m=\u001b[39m \u001b[43mCocoDetection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_directory_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTRAIN_DIRECTORY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_processor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m VAL_DATASET \u001b[38;5;241m=\u001b[39m CocoDetection(\n\u001b[1;32m     40\u001b[0m     image_directory_path\u001b[38;5;241m=\u001b[39mVAL_DIRECTORY, \n\u001b[1;32m     41\u001b[0m     image_processor\u001b[38;5;241m=\u001b[39mimage_processor, \n\u001b[1;32m     42\u001b[0m     train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     43\u001b[0m TEST_DATASET \u001b[38;5;241m=\u001b[39m CocoDetection(\n\u001b[1;32m     44\u001b[0m     image_directory_path\u001b[38;5;241m=\u001b[39mTEST_DIRECTORY, \n\u001b[1;32m     45\u001b[0m     image_processor\u001b[38;5;241m=\u001b[39mimage_processor, \n\u001b[1;32m     46\u001b[0m     train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
            "Cell \u001b[0;32mIn[3], line 21\u001b[0m, in \u001b[0;36mCocoDetection.__init__\u001b[0;34m(self, image_directory_path, image_processor, train)\u001b[0m\n\u001b[1;32m     19\u001b[0m annotation_file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(image_directory_path, ANNOTATION_FILE_NAME)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(annotation_file_path)\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mCocoDetection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mimage_directory_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mannotation_file_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_processor \u001b[38;5;241m=\u001b[39m image_processor\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/datasets/coco.py:36\u001b[0m, in \u001b[0;36mCocoDetection.__init__\u001b[0;34m(self, root, annFile, transform, target_transform, transforms)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(root, transforms, transform, target_transform)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpycocotools\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcoco\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m COCO\n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoco \u001b[38;5;241m=\u001b[39m \u001b[43mCOCO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mannFile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoco\u001b[38;5;241m.\u001b[39mimgs\u001b[38;5;241m.\u001b[39mkeys()))\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pycocotools/coco.py:81\u001b[0m, in \u001b[0;36mCOCO.__init__\u001b[0;34m(self, annotation_file)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloading annotations into memory...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     80\u001b[0m tic \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 81\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mannotation_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     82\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(dataset)\u001b[38;5;241m==\u001b[39m\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mannotation file format \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m not supported\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mtype\u001b[39m(dataset))\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'sketch_web_ui_dataset/train/images/annotations.coco.json'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torchvision\n",
        "\n",
        "\n",
        "# settings\n",
        "ANNOTATION_FILE_NAME = \"annotations.coco.json\"\n",
        "TRAIN_DIRECTORY = os.path.join('sketch_web_ui_dataset', \"train\",'images')\n",
        "VAL_DIRECTORY = os.path.join('sketch_web_ui_dataset', \"train\",'images')\n",
        "TEST_DIRECTORY = os.path.join('sketch_web_ui_dataset', \"test\",'images')\n",
        "\n",
        "\n",
        "class CocoDetection(torchvision.datasets.CocoDetection):\n",
        "    def __init__(\n",
        "        self, \n",
        "        image_directory_path: str, \n",
        "        image_processor, \n",
        "        train: bool = True\n",
        "    ):\n",
        "        annotation_file_path = os.path.join(image_directory_path, ANNOTATION_FILE_NAME)\n",
        "        print(annotation_file_path)\n",
        "        super(CocoDetection, self).__init__(image_directory_path, annotation_file_path)\n",
        "        self.image_processor = image_processor\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        images, annotations = super(CocoDetection, self).__getitem__(idx)        \n",
        "        image_id = self.ids[idx]\n",
        "        annotations = {'image_id': image_id, 'annotations': annotations}\n",
        "        encoding = self.image_processor(images=images, annotations=annotations, return_tensors=\"pt\")\n",
        "        pixel_values = encoding[\"pixel_values\"].squeeze()\n",
        "        target = encoding[\"labels\"][0]\n",
        "\n",
        "        return pixel_values, target\n",
        "\n",
        "\n",
        "TRAIN_DATASET = CocoDetection(\n",
        "    image_directory_path=TRAIN_DIRECTORY, \n",
        "    image_processor=image_processor, \n",
        "    train=True)\n",
        "VAL_DATASET = CocoDetection(\n",
        "    image_directory_path=VAL_DIRECTORY, \n",
        "    image_processor=image_processor, \n",
        "    train=False)\n",
        "TEST_DATASET = CocoDetection(\n",
        "    image_directory_path=TEST_DIRECTORY, \n",
        "    image_processor=image_processor, \n",
        "    train=False)\n",
        "\n",
        "print(\"Number of training examples:\", len(TRAIN_DATASET))\n",
        "print(\"Number of validation examples:\", len(VAL_DATASET))\n",
        "print(\"Number of test examples:\", len(TEST_DATASET))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee584fc7",
      "metadata": {
        "id": "ee584fc7"
      },
      "source": [
        "## Visualize data entry"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31a8f2f3",
      "metadata": {
        "id": "31a8f2f3"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import cv2\n",
        "import numpy as np\n",
        "import supervision as sv\n",
        "\n",
        "\n",
        "\n",
        "# select random image\n",
        "image_ids = TRAIN_DATASET.coco.getImgIds()\n",
        "image_id = random.choice(image_ids)\n",
        "print('Image #{}'.format(image_id))\n",
        "\n",
        "# load image and annotatons \n",
        "image = TRAIN_DATASET.coco.loadImgs(image_id)[0]\n",
        "annotations = TRAIN_DATASET.coco.imgToAnns[image_id]\n",
        "image_path = os.path.join(TRAIN_DATASET.root, image['file_name'])\n",
        "image = cv2.imread(image_path)\n",
        "\n",
        "# annotate\n",
        "detections = sv.Detections.from_coco_annotations(coco_annotation=annotations)\n",
        "\n",
        "# we will use id2label function for training\n",
        "categories = TRAIN_DATASET.coco.cats\n",
        "id2label = {k: v['name'] for k,v in categories.items()}\n",
        "\n",
        "labels = [\n",
        "    f\"{id2label[class_id]}\" \n",
        "    for _, _, class_id, _ \n",
        "    in detections\n",
        "]\n",
        "\n",
        "box_annotator = sv.BoxAnnotator()\n",
        "frame = box_annotator.annotate(scene=image, detections=detections, labels=labels)\n",
        "\n",
        "%matplotlib inline  \n",
        "sv.show_frame_in_notebook(image, (16, 16))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a67b412",
      "metadata": {
        "id": "4a67b412"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def collate_fn(batch):\n",
        "    # DETR authors employ various image sizes during training, making it not possible \n",
        "    # to directly batch together images. Hence they pad the images to the biggest \n",
        "    # resolution in a given batch, and create a corresponding binary pixel_mask \n",
        "    # which indicates which pixels are real/which are padding\n",
        "    pixel_values = [item[0] for item in batch]\n",
        "    encoding = image_processor.pad(pixel_values, return_tensors=\"pt\")\n",
        "    labels = [item[1] for item in batch]\n",
        "    return {\n",
        "        'pixel_values': encoding['pixel_values'],\n",
        "        'pixel_mask': encoding['pixel_mask'],\n",
        "        'labels': labels\n",
        "    }\n",
        "\n",
        "TRAIN_DATALOADER = DataLoader(dataset=TRAIN_DATASET, collate_fn=collate_fn, batch_size=4, shuffle=True)\n",
        "# VAL_DATALOADER = DataLoader(dataset=VAL_DATASET, collate_fn=collate_fn, batch_size=4)\n",
        "TEST_DATALOADER = DataLoader(dataset=TEST_DATASET, collate_fn=collate_fn, batch_size=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ce28475",
      "metadata": {
        "id": "3ce28475"
      },
      "source": [
        "## Train model with PyTorch Lightning\n",
        "\n",
        "**NOTE:** Here we define a regular PyTorch dataset. Each item of the dataset is an image and corresponding annotations. Torchvision already provides a `CocoDetection` dataset, which we can use. We only add a feature extractor (`DetrImageProcessor`) to resize + normalize the images, and to turn the annotations (which are in COCO format) in the format that DETR expects. It will also resize the annotations accordingly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8d693f1",
      "metadata": {
        "id": "b8d693f1"
      },
      "outputs": [],
      "source": [
        "import pytorch_lightning as pl\n",
        "from transformers import DetrForObjectDetection\n",
        "import torch\n",
        "\n",
        "\n",
        "class Detr(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, lr, lr_backbone, weight_decay):\n",
        "        super().__init__()\n",
        "        self.model = DetrForObjectDetection.from_pretrained(\n",
        "            pretrained_model_name_or_path=CHECKPOINT, \n",
        "            num_labels=len(id2label),\n",
        "            ignore_mismatched_sizes=True\n",
        "        )\n",
        "        \n",
        "        self.lr = lr\n",
        "        self.lr_backbone = lr_backbone\n",
        "        self.weight_decay = weight_decay\n",
        "\n",
        "    def forward(self, pixel_values, pixel_mask):\n",
        "        return self.model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n",
        "\n",
        "    def common_step(self, batch, batch_idx):\n",
        "        pixel_values = batch[\"pixel_values\"]\n",
        "        pixel_mask = batch[\"pixel_mask\"]\n",
        "        labels = [{k: v.to(self.device) for k, v in t.items()} for t in batch[\"labels\"]]\n",
        "\n",
        "        outputs = self.model(pixel_values=pixel_values, pixel_mask=pixel_mask, labels=labels)\n",
        "\n",
        "        loss = outputs.loss\n",
        "        loss_dict = outputs.loss_dict\n",
        "\n",
        "        return loss, loss_dict\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        loss, loss_dict = self.common_step(batch, batch_idx)     \n",
        "        # logs metrics for each training_step, and the average across the epoch\n",
        "        self.log(\"training_loss\", loss)\n",
        "        for k,v in loss_dict.items():\n",
        "            self.log(\"train_\" + k, v.item())\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        loss, loss_dict = self.common_step(batch, batch_idx)     \n",
        "        self.log(\"validation/loss\", loss)\n",
        "        for k, v in loss_dict.items():\n",
        "            self.log(\"validation_\" + k, v.item())\n",
        "            \n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        # DETR authors decided to use different learning rate for backbone\n",
        "        # you can learn more about it here: \n",
        "        # - https://github.com/facebookresearch/detr/blob/3af9fa878e73b6894ce3596450a8d9b89d918ca9/main.py#L22-L23\n",
        "        # - https://github.com/facebookresearch/detr/blob/3af9fa878e73b6894ce3596450a8d9b89d918ca9/main.py#L131-L139\n",
        "        param_dicts = [\n",
        "            {\n",
        "                \"params\": [p for n, p in self.named_parameters() if \"backbone\" not in n and p.requires_grad]},\n",
        "            {\n",
        "                \"params\": [p for n, p in self.named_parameters() if \"backbone\" in n and p.requires_grad],\n",
        "                \"lr\": self.lr_backbone,\n",
        "            },\n",
        "        ]\n",
        "        return torch.optim.AdamW(param_dicts, lr=self.lr, weight_decay=self.weight_decay)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return TRAIN_DATALOADER\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return VAL_DATALOADER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7611123",
      "metadata": {
        "id": "b7611123"
      },
      "outputs": [],
      "source": [
        "%cd {HOME}\n",
        "%mkdir lightning_logs\n",
        "%load_ext tensorboard\n",
        "# %reload_ext tensorboard\n",
        "\n",
        "%tensorboard --logdir lightning_logs/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83a76ee8",
      "metadata": {
        "id": "83a76ee8"
      },
      "outputs": [],
      "source": [
        "model = Detr(lr=1e-4, lr_backbone=1e-5, weight_decay=1e-4)\n",
        "\n",
        "batch = next(iter(TRAIN_DATALOADER))\n",
        "# print(batch)\n",
        "outputs = model(pixel_values=batch['pixel_values'], pixel_mask=batch['pixel_mask'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25cf9422",
      "metadata": {
        "id": "25cf9422"
      },
      "outputs": [],
      "source": [
        "outputs.logits.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ac4581c",
      "metadata": {
        "id": "6ac4581c"
      },
      "outputs": [],
      "source": [
        "from pytorch_lightning import Trainer\n",
        "\n",
        "%cd {HOME}\n",
        "\n",
        "# settings\n",
        "MAX_EPOCHS = 5\n",
        "\n",
        "# pytorch_lightning < 2.0.0\n",
        "# trainer = Trainer(gpus=1, max_epochs=MAX_EPOCHS, gradient_clip_val=0.1, accumulate_grad_batches=8, log_every_n_steps=5)\n",
        "\n",
        "# pytorch_lightning >= 2.0.0\n",
        "trainer = Trainer(devices=1, \n",
        "                  accelerator=\"cpu\", \n",
        "                  max_epochs=MAX_EPOCHS, \n",
        "                  gradient_clip_val=0.1, \n",
        "                  accumulate_grad_batches=4, \n",
        "                  log_every_n_steps=5)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b1769e7",
      "metadata": {
        "id": "4b1769e7"
      },
      "outputs": [],
      "source": [
        "trainer.fit(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dea65268",
      "metadata": {
        "id": "dea65268"
      },
      "outputs": [],
      "source": [
        "model.to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16f2aabb",
      "metadata": {
        "id": "16f2aabb"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# utils\n",
        "categories = TEST_DATASET.coco.cats\n",
        "id2label = {k: v['name'] for k,v in categories.items()}\n",
        "box_annotator = sv.BoxAnnotator()\n",
        "\n",
        "# select random image\n",
        "image_ids = TEST_DATASET.coco.getImgIds()\n",
        "image_id = random.choice(image_ids)\n",
        "print('Image #{}'.format(image_id))\n",
        "\n",
        "# load image and annotatons \n",
        "image = TEST_DATASET.coco.loadImgs(image_id)[0]\n",
        "annotations = TEST_DATASET.coco.imgToAnns[image_id]\n",
        "image_path = os.path.join(TEST_DATASET.root, image['file_name'])\n",
        "image = cv2.imread(image_path)\n",
        "\n",
        "# annotate\n",
        "detections = sv.Detections.from_coco_annotations(coco_annotation=annotations)\n",
        "labels = [f\"{id2label[class_id]}\" for _, _, class_id, _ in detections]\n",
        "frame = box_annotator.annotate(scene=image.copy(), detections=detections, labels=labels)\n",
        "\n",
        "print('ground truth')\n",
        "%matplotlib inline  \n",
        "sv.show_frame_in_notebook(frame, (16, 16))\n",
        "\n",
        "# inference\n",
        "with torch.no_grad():\n",
        "\n",
        "    # load image and predict\n",
        "    inputs = image_processor(images=image, return_tensors='pt').to(DEVICE)\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "    # post-process\n",
        "    target_sizes = torch.tensor([image.shape[:2]]).to(DEVICE)\n",
        "    results = image_processor.post_process_object_detection(\n",
        "        outputs=outputs, \n",
        "        threshold=CONFIDENCE_TRESHOLD, \n",
        "        target_sizes=target_sizes\n",
        "    )[0]\n",
        "\n",
        "# annotate\n",
        "detections = sv.Detections.from_transformers(transformers_results=results)\n",
        "labels = [f\"{id2label[class_id]} {confidence:.2f}\" for _, confidence, class_id, _ in detections]\n",
        "frame = box_annotator.annotate(scene=image.copy(), detections=detections, labels=labels)\n",
        "\n",
        "print('detections')\n",
        "%matplotlib inline  \n",
        "sv.show_frame_in_notebook(frame, (16, 16))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e57b765f",
      "metadata": {
        "id": "e57b765f"
      },
      "source": [
        "## Evaluation on test dataset\n",
        "\n",
        "Finally, we evaluate the model on the `TEST_DATASET`. For this we make use of the `CocoEvaluator` class available in a tiny [PyPi package](https://github.com/NielsRogge/coco-eval) made by [Niels Rogge](https://github.com/NielsRogge) . This class is entirely based on the [original evaluator](https://github.com/facebookresearch/detr/blob/main/datasets/coco_eval.py) class used by the DETR authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9d43a7e",
      "metadata": {
        "id": "d9d43a7e"
      },
      "outputs": [],
      "source": [
        "def convert_to_xywh(boxes):\n",
        "    xmin, ymin, xmax, ymax = boxes.unbind(1)\n",
        "    return torch.stack((xmin, ymin, xmax - xmin, ymax - ymin), dim=1)\n",
        "\n",
        "def prepare_for_coco_detection(predictions):\n",
        "    coco_results = []\n",
        "    for original_id, prediction in predictions.items():\n",
        "        if len(prediction) == 0:\n",
        "            continue\n",
        "\n",
        "        boxes = prediction[\"boxes\"]\n",
        "        boxes = convert_to_xywh(boxes).tolist()\n",
        "        scores = prediction[\"scores\"].tolist()\n",
        "        labels = prediction[\"labels\"].tolist()\n",
        "\n",
        "        coco_results.extend(\n",
        "            [\n",
        "                {\n",
        "                    \"image_id\": original_id,\n",
        "                    \"category_id\": labels[k],\n",
        "                    \"bbox\": box,\n",
        "                    \"score\": scores[k],\n",
        "                }\n",
        "                for k, box in enumerate(boxes)\n",
        "            ]\n",
        "        )\n",
        "    return coco_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb40c17d",
      "metadata": {
        "id": "fb40c17d"
      },
      "outputs": [],
      "source": [
        "# TEST_DATASET.transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f6ab2e5",
      "metadata": {
        "id": "4f6ab2e5"
      },
      "outputs": [],
      "source": [
        "from coco_eval import CocoEvaluator\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "evaluator = CocoEvaluator(coco_gt=TEST_DATASET.coco, iou_types=[\"bbox\"])\n",
        "\n",
        "print(\"Running evaluation...\")\n",
        "\n",
        "for idx, batch in enumerate(tqdm(TEST_DATALOADER)):\n",
        "    pixel_values = batch[\"pixel_values\"].to(DEVICE)\n",
        "    pixel_mask = batch[\"pixel_mask\"].to(DEVICE)\n",
        "    labels = [{k: v.to(DEVICE) for k, v in t.items()} for t in batch[\"labels\"]]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n",
        "\n",
        "    orig_target_sizes = torch.stack([target[\"orig_size\"] for target in labels], dim=0)\n",
        "    results = image_processor.post_process_object_detection(outputs, target_sizes=orig_target_sizes)\n",
        "\n",
        "    predictions = {target['image_id'].item(): output for target, output in zip(labels, results)}\n",
        "    predictions = prepare_for_coco_detection(predictions)\n",
        "    evaluator.update(predictions)\n",
        "\n",
        "evaluator.synchronize_between_processes()\n",
        "evaluator.accumulate()\n",
        "evaluator.summarize()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a726db1a",
      "metadata": {
        "id": "a726db1a"
      },
      "source": [
        "## Save and load model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d8852d4",
      "metadata": {
        "id": "3d8852d4"
      },
      "outputs": [],
      "source": [
        "MODEL_PATH = os.path.join(HOME, 'custom-model-DETR')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdcc41e3",
      "metadata": {
        "id": "bdcc41e3"
      },
      "outputs": [],
      "source": [
        "model.model.save_pretrained(MODEL_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20210d42",
      "metadata": {
        "id": "20210d42"
      },
      "outputs": [],
      "source": [
        "model = DetrForObjectDetection.from_pretrained(MODEL_PATH)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfa59efd",
      "metadata": {
        "id": "bfa59efd"
      },
      "outputs": [],
      "source": [
        "[{k: v.to(DEVICE) for k, v in t.items()} for t in batch[\"labels\"]][0].keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72dd743f",
      "metadata": {
        "id": "72dd743f"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa1e1d3f",
      "metadata": {
        "id": "aa1e1d3f"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e73a5e50",
      "metadata": {
        "id": "e73a5e50"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbd26346",
      "metadata": {
        "id": "bbd26346"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6256893b",
      "metadata": {
        "id": "6256893b"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "593ad516",
      "metadata": {
        "id": "593ad516"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6992b716",
      "metadata": {
        "id": "6992b716"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5dfe7d97",
      "metadata": {
        "id": "5dfe7d97"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae3d5287",
      "metadata": {
        "id": "ae3d5287"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f48258ab",
      "metadata": {
        "id": "f48258ab"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0b2ce5d",
      "metadata": {
        "id": "b0b2ce5d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8727b07d",
      "metadata": {
        "id": "8727b07d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d308b588",
      "metadata": {
        "id": "d308b588"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}